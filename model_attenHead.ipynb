{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from torch.optim import AdamW,Adam,SGD\n",
    "from torch.nn.utils import clip_grad_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=12, p=1)\n",
    "data_train = ImgDataset('omniglot-py/images_background',n,k,64,transform)\n",
    "data_train = DataLoader(data_train,b,drop_last=True)\n",
    "data_val = ImgDataset('omniglot-py/images_evaluation',n,k,16,transform)\n",
    "data_val = DataLoader(data_val,b,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "clip = 1e-2\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttenMeta_attenHead(d, dropout,layers,False).to('cuda')\n",
    "#model.input_target.to('cuda')\n",
    "paras = model.parameters()\n",
    "#opt = AdamW(paras,lr = lr,amsgrad=True)\n",
    "#opt = Adam(paras,lr = lr)\n",
    "opt = SGD(paras,lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:4.510920548843125, val_loss:1.9908385806613498\n",
      "epoch:1, train_loss:2.2924488257553617, val_loss:1.9231614801618788\n",
      "epoch:2, train_loss:1.952532806638944, val_loss:1.783364216486613\n",
      "epoch:3, train_loss:1.835645998938609, val_loss:1.7729239993625217\n",
      "epoch:4, train_loss:1.7732165930634838, val_loss:1.722715695699056\n",
      "epoch:5, train_loss:1.7125566834110324, val_loss:1.6746583249833848\n",
      "epoch:6, train_loss:1.6756452544260834, val_loss:1.5749212106068928\n",
      "epoch:7, train_loss:1.6203236377845376, val_loss:1.499140527513292\n",
      "epoch:8, train_loss:1.5948008258463973, val_loss:1.5937618414560955\n",
      "epoch:9, train_loss:1.5606713497032554, val_loss:1.6435925165812175\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Training #\n",
    "    for i,x in enumerate(data_train):\n",
    "        x = x.to('cuda:0')\n",
    "        loss = model(x,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(data_val):\n",
    "            x = x.to('cuda:0')\n",
    "            loss = model(x,True)\n",
    "            val_loss += loss.item()      \n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:1.5266561427358853, val_loss:1.523974339167277\n",
      "epoch:1, train_loss:1.4768266152527372, val_loss:1.4817105399237738\n",
      "epoch:2, train_loss:1.4290218131016876, val_loss:1.4320711294809978\n",
      "epoch:3, train_loss:1.3956596851348877, val_loss:1.4593888653649225\n",
      "epoch:4, train_loss:1.3644714153419106, val_loss:1.3050066100226507\n",
      "epoch:5, train_loss:1.3220225128076843, val_loss:1.2300921810997858\n",
      "epoch:6, train_loss:1.2874030258695959, val_loss:1.211440298292372\n",
      "epoch:7, train_loss:1.2681044986692525, val_loss:1.1783495214250352\n",
      "epoch:8, train_loss:1.21741907677408, val_loss:1.3209819793701172\n",
      "epoch:9, train_loss:1.1913544788198955, val_loss:1.104235914018419\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Training #\n",
    "    for i,x in enumerate(data_train):\n",
    "        x = x.to('cuda:0')\n",
    "        loss = model(x,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(data_val):\n",
    "            x = x.to('cuda:0')\n",
    "            loss = model(x,True)\n",
    "            val_loss += loss.item()      \n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:1.135080286001755, val_loss:1.084055436982049\n",
      "epoch:1, train_loss:1.119110508490417, val_loss:1.0066686073939006\n",
      "epoch:2, train_loss:1.0979033051911047, val_loss:0.9764848815070258\n",
      "epoch:3, train_loss:1.0535200713044506, val_loss:0.9534618589613173\n",
      "epoch:4, train_loss:1.0138089808367066, val_loss:1.0035087664922078\n",
      "epoch:5, train_loss:0.9944280325356176, val_loss:0.7994795507854886\n",
      "epoch:6, train_loss:0.9606066932112484, val_loss:0.7279080814785428\n",
      "epoch:7, train_loss:0.9326634275711189, val_loss:0.7892558972040812\n",
      "epoch:8, train_loss:0.9196714183031502, val_loss:0.780679186185201\n",
      "epoch:9, train_loss:0.8786304552676314, val_loss:0.7072426213158501\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Training #\n",
    "    for i,x in enumerate(data_train):\n",
    "        x = x.to('cuda:0')\n",
    "        loss = model(x,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(data_val):\n",
    "            x = x.to('cuda:0')\n",
    "            loss = model(x,True)\n",
    "            val_loss += loss.item()      \n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.8832847890207323, val_loss:0.813315643204583\n",
      "epoch:1, train_loss:0.8646156929307065, val_loss:0.75420081615448\n",
      "epoch:2, train_loss:0.820683185326851, val_loss:0.7341064347161187\n",
      "epoch:3, train_loss:0.8123814140335989, val_loss:0.582833488782247\n",
      "epoch:4, train_loss:0.7771413972822286, val_loss:0.7210632827546861\n",
      "epoch:5, train_loss:0.7945660716396267, val_loss:0.6422441535525851\n",
      "epoch:6, train_loss:0.7617271956750902, val_loss:0.5970191293292575\n",
      "epoch:7, train_loss:0.773408008834063, val_loss:0.7812756962246366\n",
      "epoch:8, train_loss:0.7377066622346135, val_loss:0.6114318635728624\n",
      "epoch:9, train_loss:0.740290371038146, val_loss:0.6794893079333835\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Training #\n",
    "    for i,x in enumerate(data_train):\n",
    "        x = x.to('cuda:0')\n",
    "        loss = model(x,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(data_val):\n",
    "            x = x.to('cuda:0')\n",
    "            loss = model(x,True)\n",
    "            val_loss += loss.item()      \n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.7197074496139915, val_loss:0.6363354788886176\n",
      "epoch:1, train_loss:0.7055658112138005, val_loss:0.5683341291215684\n",
      "epoch:2, train_loss:0.7055364362264084, val_loss:0.7777285575866699\n",
      "epoch:3, train_loss:0.6889989709450026, val_loss:0.5662243233786689\n",
      "epoch:4, train_loss:0.6819544404239978, val_loss:0.6149243646197848\n",
      "epoch:5, train_loss:0.6636990480503794, val_loss:0.696637299325731\n",
      "epoch:6, train_loss:0.6457145840434705, val_loss:0.5178422398037381\n",
      "epoch:7, train_loss:0.6458548554929636, val_loss:0.4678482148382399\n",
      "epoch:8, train_loss:0.6206569277634055, val_loss:0.5903928147421943\n",
      "epoch:9, train_loss:0.6220385457499552, val_loss:0.42968428797192043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "opt.zero_grad()\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    np.random.seed()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Training #\n",
    "    for i,x in enumerate(data_train):\n",
    "        x = x.to('cuda:0')\n",
    "        loss = model(x,True)\n",
    "        loss.backward()\n",
    "        clip_grad_value_(paras,clip)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # evaluating #\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(data_val):\n",
    "            x = x.to('cuda:0')\n",
    "            loss = model(x,True)\n",
    "            val_loss += loss.item()      \n",
    "    print('epoch:{}, train_loss:{}, val_loss:{}'.format(epoch,train_loss/i,val_loss/j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8438, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(data_val):\n",
    "        x = x.to('cuda:0')\n",
    "        yhat = model(x,False)\n",
    "        acc += torch.sum(yhat.argmax(-1).flatten() == model.y)/b/n\n",
    "print(acc/(j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3704, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for j,data in enumerate(data_val):\n",
    "        x = x.to('cuda:0')\n",
    "        yhat = model(x,False)\n",
    "        acc += torch.sum(yhat.argmax(-1).flatten() == model.y)/b/n\n",
    "print(acc/j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16666666666666666, 1.791759469228055)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/n, -np.log(1/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.9969, -0.9699, -1.0318, -0.3555, -1.4680,  2.9855],\n",
       "        [-5.1179,  0.7265,  0.6895,  0.6566,  0.6806, -2.0669],\n",
       "        [-5.1179,  0.7265,  0.6895,  0.6566,  0.6806, -2.0669],\n",
       "        ...,\n",
       "        [-5.1178,  0.7264,  0.6896,  0.6564,  0.6806, -2.0670],\n",
       "        [-5.1178,  0.7264,  0.6896,  0.6564,  0.6806, -2.0670],\n",
       "        [ 2.9970, -0.9698, -1.0318, -0.3555, -1.4684,  2.9854]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
